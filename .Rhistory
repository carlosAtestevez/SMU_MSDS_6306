table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther) #Actual in Columns
confusionMatrix(factor(allNYTSearchTest$Classified),factor(allNYTSearchTest$NewsOrOther))
library(tm) #text mining library provides the stopwords() function
library(tidyr)
library(plyr)
library(jsonlite)
library(dplyr)
library(tidyverse)
library(class)
library(caret)
NYTIMES_KEY = "QPw9fgKWrpDrL9kS42X42UFXLTO1lJP2"
term <- "Trump"
begin_date <- "20230101"
end_date <- "20230131"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
baseurl
initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
maxPages = 50
pages <- list()
for(i in 0:maxPages){
url_page = paste0(baseurl, "&page=", i)
print(url_page)
nytSearch <- jsonlite::fromJSON(url_page, flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(7)
}
library(tm) #text mining library provides the stopwords() function
library(tidyr)
library(plyr)
library(jsonlite)
library(dplyr)
library(tidyverse)
library(class)
library(caret)
NYTIMES_KEY = "QPw9fgKWrpDrL9kS42X42UFXLTO1lJP2"
term <- "Trump"
begin_date <- "20230101"
end_date <- "20230131"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
baseurl
initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
maxPages = 10
pages <- list()
for(i in 0:maxPages){
url_page = paste0(baseurl, "&page=", i)
print(url_page)
nytSearch <- jsonlite::fromJSON(url_page, flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(7)
}
allNYTSearch <- rbind_pages(pages)
# Visualize coverage by section
allNYTSearch %>%
group_by(response.docs.type_of_material) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()
#Make another column of News versus Other ... The labels
allNYTSearch$NewsOrOther = ifelse(allNYTSearch$response.docs.type_of_material == "News","News","Other")
# Visualize coverage of News or Other
allNYTSearch[!is.na(allNYTSearch$NewsOrOther),] %>%
group_by(NewsOrOther) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=NewsOrOther, fill=NewsOrOther), stat = "identity") + coord_flip()
#Training the model
nr_percentage = 0.7
nr_observations = dim(allNYTSearch)[1]
set.seed(2)
trainInd = sample(seq(1,dim(allNYTSearch)[1],1),round(nr_percentage * nr_observations))
allNYTSearchTrain = allNYTSearch[trainInd,]
allNYTSearchTest = allNYTSearch[-trainInd,]
Pnews_word = function(key_word, trainingSet, alphaLaplace = 1, betaLaplace = 1) # alpha and beta are for laplace smoothing
{
trainingSet$response.docs.snippet = unlist(str_replace_all(trainingSet$response.docs.snippet,"[^[:alnum:] ]", "")) #Take out all but alpha numeric characters from training headlines
#print(key_word)
NewsGroup = trainingSet[trainingSet$NewsOrOther == "News",]
OtherGroup = trainingSet[trainingSet$NewsOrOther == "Other",]
pNews = dim(NewsGroup)[1] / (dim(NewsGroup)[1] + dim(OtherGroup)[1])
pOther = 1 - pNews
pKWGivenNews = (length(str_which(NewsGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(NewsGroup)[1]+betaLaplace)
pKWGivenOther = (length(str_which(OtherGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(OtherGroup)[1]+betaLaplace)
pKW = length(str_which(trainingSet$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))/dim(trainingSet)[1]
pNewsGivenKW = pKWGivenNews*pNews/pKW
pOtherGivenKW = pKWGivenOther*pOther/pKW
return(pNewsGivenKW)
}
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
articleScoreNews = 1;
articleScoreOther = 1;
#print(allNYTSearchTest[i,]$response.docs.snippet)
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.snippet,"[^[:alnum:] ]", ""), stringr::boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b")
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
{
articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
}
theScoreHolderNews[i] = articleScoreNews
theScoreHolderOther[i] = articleScoreOther
}
# Classify the aricle as News or Other based on a given piece of information from the article.
allNYTSearchTest$Classified = ifelse(theScoreHolderNews > theScoreHolderOther,"News","Other")
#Confusion Matrix
table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther) #Actual in Columns
confusionMatrix(factor(allNYTSearchTest$Classified),factor(allNYTSearchTest$NewsOrOther))
library(tm) #text mining library provides the stopwords() function
library(tidyr)
library(plyr)
library(jsonlite)
library(dplyr)
library(tidyverse)
library(class)
library(caret)
NYTIMES_KEY = "QPw9fgKWrpDrL9kS42X42UFXLTO1lJP2"
term <- "Trump"
begin_date <- "20230101"
end_date <- "20230131"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
baseurl
initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
maxPages = 10
pages <- list()
for(i in 0:maxPages){
url_page = paste0(baseurl, "&page=", i)
print(url_page)
nytSearch <- jsonlite::fromJSON(url_page, flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(7)
}
allNYTSearch <- rbind_pages(pages)
# Visualize coverage by section
allNYTSearch %>%
group_by(response.docs.type_of_material) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()
#Make another column of News versus Other ... The labels
allNYTSearch$NewsOrOther = ifelse(allNYTSearch$response.docs.type_of_material == "News","News","Other")
# Visualize coverage of News or Other
allNYTSearch[!is.na(allNYTSearch$NewsOrOther),] %>%
group_by(NewsOrOther) %>%
dplyr::summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=NewsOrOther, fill=NewsOrOther), stat = "identity") + coord_flip()
#Training the model
nr_percentage = 0.7
nr_observations = dim(allNYTSearch)[1]
set.seed(2)
trainInd = sample(seq(1,dim(allNYTSearch)[1],1),round(nr_percentage * nr_observations))
allNYTSearchTrain = allNYTSearch[trainInd,]
allNYTSearchTest = allNYTSearch[-trainInd,]
Pnews_word = function(key_word, trainingSet, alphaLaplace = 1, betaLaplace = 1) # alpha and beta are for laplace smoothing
{
trainingSet$response.docs.headline.main = unlist(str_replace_all(trainingSet$response.docs.headline.main,"[^[:alnum:] ]", "")) #Take out all but alpha numeric characters from training headlines
#print(key_word)
NewsGroup = trainingSet[trainingSet$NewsOrOther == "News",]
OtherGroup = trainingSet[trainingSet$NewsOrOther == "Other",]
pNews = dim(NewsGroup)[1] / (dim(NewsGroup)[1] + dim(OtherGroup)[1])
pOther = 1 - pNews
pKWGivenNews = (length(str_which(NewsGroup$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(NewsGroup)[1]+betaLaplace)
pKWGivenOther = (length(str_which(OtherGroup$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(OtherGroup)[1]+betaLaplace)
pKW = length(str_which(trainingSet$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))/dim(trainingSet)[1]
pNewsGivenKW = pKWGivenNews*pNews/pKW
pOtherGivenKW = pKWGivenOther*pOther/pKW
return(pNewsGivenKW)
}
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
articleScoreNews = 1;
articleScoreOther = 1;
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.headline.main,"[^[:alnum:] ]", ""), stringr::boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b")
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
{
articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
}
theScoreHolderNews[i] = articleScoreNews
theScoreHolderOther[i] = articleScoreOther
}
# Classify the aricle as News or Other based on a given piece of information from the article.
allNYTSearchTest$Classified = ifelse(theScoreHolderNews > theScoreHolderOther,"News","Other")
#Confusion Matrix
table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther) #Actual in Columns
confusionMatrix(factor(allNYTSearchTest$Classified),factor(allNYTSearchTest$NewsOrOther))
file.choose()
dataset <- read.csv(""C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data,header=FALSE,sep=";")
dataset <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data",header=FALSE,sep=";")
head(dataset)
dataset <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data",header=FALSE,sep=";")
names <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.names",header=FALSE,sep=";")
names(dataset) <- sapply((1:nrow(names)),function(i) toString(names[i,1]))
dataset <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data",header=FALSE,sep=";")
names <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.names",header=FALSE,sep=";")
head(names)
library(readr)
library(readr)
#data_raw <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data",header=FALSE,sep=";")
#names <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.names",header=FALSE,sep=";")
#Data File
data_raw <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data", header = F)
#Names File
data_raw_names <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.names", header = FALSE)
data_raw_names <- data_raw_names[-(1:30),]
data_raw_names <- as.data.frame(data_raw_names)
library(dplyr)
library(tidyr)
data_raw_names <- data_raw_names %>%
separate(data_raw_names, c("Variable", "Type"), sep = ":")
#Assigning Name to Dataset
names(data_raw) <- data_raw_names$Variable
names(data_raw)[is.na(names(data_raw))] <- "classes"
data <- data_raw
head(data)
View(data)
str(data)
data_text <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\SMSSpamCollection", sep="\t", header=F, colClasses="character", quote="")
library(readr)
#data_raw <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data",header=FALSE,sep=";")
#names <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.names",header=FALSE,sep=";")
data_text <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\smsspamcollection\\SMSSpamCollection", sep="\t", header=F, colClasses="character", quote="")
v
str(data_text)
data_text <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\smsspamcollection\\SMSSpamCollection", sep="\t", header=F, colClasses="character", quote="")
data_text <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\smsspamcollection\\SMSSpamCollection", sep="\t", header=F, colClasses="character", quote="")
str(data_text)
data_text <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\smsspamcollection\\SMSSpamCollection", sep="\t", header=F, colClasses="character", quote="")
colnames(data_text) <- c("Class", "Text")
colnames(data_text)
data_text$Class <- factor(data_text$Class)
prop.table(table(data_text$Class))
library(readr)
library(tm)
library(SnowballC)
install.packages("SnowballC")
library(readr)
library(tm)
library(SnowballC)
#data_raw <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data",header=FALSE,sep=";")
#names <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.names",header=FALSE,sep=";")
data_text <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\smsspamcollection\\SMSSpamCollection", sep="\t", header=F, colClasses="character", quote="")
colnames(data_text) <- c("Class", "Text")
colnames(data_text)
data_text$Class <- factor(data_text$Class)
prop.table(table(data_text$Class))
library(readr)
library(tm)
library(SnowballC)
#data_raw <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.data",header=FALSE,sep=";")
#names <- read.csv("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\spambase\\spambase.names",header=FALSE,sep=";")
data_text <- read.delim("C:\\Users\\cestevez\\Dropbox\\Cloud PC\\Thinkpad\\Thinkpad Desktop\\Master Data Science SMU\\Class_Sessions\\Data Science Sessions\\Repository\\SMU_MSDS_6306\\session_7\\smsspamcollection\\SMSSpamCollection", sep="\t", header=F, colClasses="character", quote="")
colnames(data_text) <- c("Class", "Text")
colnames(data_text)
data_text$Class <- factor(data_text$Class)
prop.table(table(data_text$Class))
corpus = VCorpus(VectorSource(data_text$Text))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
as.character(corpus[[1]])
dtm = DocumentTermMatrix(corpus)
dtm
vIEW(dtm)
View(dtm)
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
inspect(dtm[40:50, 10:15])
dtm
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
# Apply the convert_count function to get final training and testing DTMs
datasetNB <- apply(dtm, 2, convert_count)
dataset = as.data.frame(as.matrix(datasetNB))
freq<- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
tail(freq, 10)
View(dataset)
head(dataset)
str(dataset)
findFreqTerms(dtm, lowfreq=60) #identifying terms that appears frequently
freq<- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
tail(freq, 10)
findFreqTerms(dtm, lowfreq=60) #identifying terms that appears frequently
Sys.getenv()
getwd()
setwd("C:/Users/cestevez/Dropbox/Cloud PC/Thinkpad/Thinkpad Desktop/Master Data Science SMU/Class_Sessions/Data Science Sessions/Repository/SMU_MSDS_6306/session_8")
getwd()
setwd("C:/Users/cestevez/Dropbox/Cloud PC/Thinkpad/Thinkpad Desktop/Master Data Science SMU/Class_Sessions/Data Science Sessions/Repository/SMU_MSDS_6306")
library(tidyverse)
library(stringr)
df_beers = read.csv("/session_8/Beers.csv",header = TRUE)
library(tidyverse)
library(stringr)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
library(tidyverse)
library(stringr)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
View(df_beers)
View(df_beers)
summary(df_beers)
count(df_beers)
n(df_beers)
str(df_beers)
View(df_beers,df_breweries
)
str(df_beers)
View(df_beers,df_breweries
)
View(df_breweries)
df_breweries = read.csv("session_8/Breweries.csv",header = TRUE)
View(df_breweries)
#How many breweries are present in each state
df_breweries %>% group_by(State) %>% summarize(NumberBreweries = n())
#How many breweries are present in each state
df_sum_brew = df_breweries %>% group_by(State) %>% summarize(NumberBreweries = n())
library(tidyverse)
library(stringr)
#Reading files
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_breweries = read.csv("session_8/Breweries.csv",header = TRUE)
df_states = read.csv("session_8/States.csv",header=TRUE)
View(df_states)
str(df_beers)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_breweries_1 = read.csv("session_8/Breweries.csv",header = TRUE)
df_states = read.csv("session_8/States.csv",header=TRUE)
df_breweries_2 = merge(df_breweries_1,df_states,by="State")
View(df_breweries_2)
View(df_breweries_1)
View(df_states)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_breweries_1 = read.csv("session_8/Breweries.csv",header = TRUE)
df_states = read.csv("session_8/States.csv",header=TRUE)
df_breweries_2 = merge(df_breweries_1,df_states,by.x = "State",by.y = "Abbreviation")
View(df_breweries_2)
library(tidyverse)
library(stringr)
#Reading files
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_breweries_1 = read.csv("session_8/Breweries.csv",header = TRUE)
df_states = read.csv("session_8/States.csv",header=TRUE)
df_breweries_2 = merge(df_breweries_1,df_states,by.x = "State",by.y = "Abbreviation")
View(df_breweries_2)
str(df_beers)
#How many breweries are present in each state
df_sum_brew = df_breweries %>% group_by(State) %>% summarize(NumberBreweries = n())
View(df_states)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_breweries_1 = read.csv("session_8/Breweries.csv",header = TRUE)
df_states = read.csv("session_8/States.csv",header=TRUE)
df_breweries_2 = merge(df_breweries_1,df_states,by = "State")
View(df_breweries_2)
View(df_breweries_1)
str(df_breweries_1)
str(df_states)
str(df_breweries_1)
str(df_states)
str(df_breweries_1)
str1= "sads "
str_trim(str1)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_breweries_1 = read.csv("session_8/Breweries.csv",header = TRUE)
df_states = read.csv("session_8/States.csv",header=TRUE)
df_breweries_1$State = str_trim(df_breweries_1$State)
str(df_breweries_1)
str(df_states)
df_breweries_2 = merge(df_breweries_1,df_states,by = "State")
View(df_breweries_2)
#How many breweries are present in each state
df_sum_brew = df_breweries %>% group_by(State,Name_State) %>% summarize(NumberBreweries = n())
#How many breweries are present in each state
df_sum_brew = df_breweries_2 %>% group_by(State,Name_State) %>% summarize(NumberBreweries = n())
library(tidyverse)
library(stringr)
#Reading files
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_breweries_1 = read.csv("session_8/Breweries.csv",header = TRUE)
df_states = read.csv("session_8/States.csv",header=TRUE)
df_breweries_1$State = str_trim(df_breweries_1$State)
df_breweries_2 = merge(df_breweries_1,df_states,by = "State")
str(df_breweries_1)
str(df_states)
#How many breweries are present in each state
df_sum_brew = df_breweries_2 %>% group_by(State,Name_State) %>% summarize(NumberBreweries = n())
View(df_sum_brew)
View(df_beers)
df_merge_1 = merge(df_beers,df_breweries_2,by.x = "Brewbery_id",by.y = "Brew_ID")
df_merge_1 = merge(df_beers,df_breweries_2,by.x = "Brewery_id",by.y = "Brew_ID")
View(df_merge_1)
knitr::opts_chunk$set(echo = TRUE)
print("Hola")
iris_data = iris
library(ggthemes)
library(tidyverse)
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
iris_data
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
knitr::kable(
iris_data,
caption = "Itis data"
)
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
knitr::kable(
iris_data,
caption = "Itis data"
)
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
knitr::kable(
iris_data,
caption = "Itis data"
)
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
knitr::kable(
iris_data,
caption = "Itis data"
)
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
knitr::kable(
iris_data,
caption = "Itis data"
)
knitr::opts_chunk$set(echo = TRUE)
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
knitr::kable(
iris_data,
caption = "Itis data"
)
iris_data = iris
iris_data %>% ggplot(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+geom_point()+
labs(title = "Analysis Iris flowers",subtitle = "Analysis Iris Flower using Attrs")
knitr::kable(
iris_data,
caption = "Itis data"
)
library(ggthemes)
library(tidyverse)
library(GGally)
library(ggthemes)
library(tidyverse)
library(GGally)
library(plotly)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
df_beers = read.csv("session_8/Beers.csv",header = TRUE)
library(tidyverse)
library(stringr)
library(tidyverse)
library(stringr)
